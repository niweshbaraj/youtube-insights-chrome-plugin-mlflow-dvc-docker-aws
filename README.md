# ğŸ¬ Youtube Insights Chrome Plugin

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>

A Chrome extension that gives you instant insights on YouTube videos by analyzing their comments using an end-to-end machine learning pipeline.

Built with modern MLOps practices, the application integrates:

- âš™ï¸ **MLFlow** for experiment & model tracking & registry/deployment
- ğŸ§ª **DVC** for data versioning and pipeline
- ğŸ³ **Docker** for containerization
- ğŸ” **CI/CD pipelines** for automated deployment
- â˜ï¸ **AWS EC2, S3, and ECR** for scalable cloud infrastructure

This plugin connects to a locally running Flask API (or a cloud-deployed endpoint) to process YouTube video comments, perform sentiment analysis, and visualize keyword patternsâ€”all in real-time within the YouTube interface.

ğŸ”§ Features

- Sentiment analysis of YouTube comments

- Word cloud and keyword frequency visualizations

- Real-time insights within YouTube

- Uses YouTube Data API v3

ğŸš€ Getting Started

How to run the backend API (Flask + ML model)

How to install and use the Chrome extension locally

ğŸ” YouTube API Key Setup
Step-by-step guide to get an API key from Google Cloud Console

ğŸ³ Docker Deployment
Build and run backend container

Optional: Push to AWS ECR

âš™ï¸ CI/CD & Cloud Infra
How CI/CD is set up (GitHub Actions, etc.)

AWS deployment architecture




## ğŸ—ƒï¸ Project Organization / Folder Structure

```
â”œâ”€â”€ LICENSE            <- Open-source license if one is chosen
â”œâ”€â”€ Makefile           <- Makefile with convenience commands like `make data` or `make train`
â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
â”œâ”€â”€ client             <- Frontend code for Youtube extension
â”‚   â”œâ”€â”€ README.md       
â”‚   â”œâ”€â”€ manisfest.json        
â”‚   â”œâ”€â”€ popup.html      
â”‚   â””â”€â”€ popup.js        
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ external       <- Data from third party sources.
â”‚   â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
â”‚   â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
â”‚   â””â”€â”€ raw            <- The original, immutable data dump.
â”‚
â”œâ”€â”€ docs               <- A default mkdocs project; see www.mkdocs.org for details
â”‚
â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
â”‚
â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
â”‚                         the creator's initials, and a short `-` delimited description, e.g.
â”‚                         `1.0-jqp-initial-data-exploration`.
â”‚
â”œâ”€â”€ pyproject.toml     <- Project configuration file with package metadata for 
â”‚                         youtube_insights_chrome_plugin and configuration for tools like black
â”‚
â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
â”‚
â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
â”‚   â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
â”‚
â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
â”‚                         generated with `pip freeze > requirements.txt`
â”‚
â”œâ”€â”€ setup.cfg          <- Configuration file for flake8
â”‚
â”œâ”€â”€ server             <- Backend API code
â”‚
|    â”œâ”€â”€ app.py
|
|    â”œâ”€â”€ requirements.txt
â””â”€â”€ src                <- Source code for use in this project.
|    â”œâ”€â”€ data                   
|        â”œâ”€â”€ data_ingestion.py        <- Code to download and generate data
|        â”‚
|        â”œâ”€â”€ data_preprocessing.py    <- Code to process data
|        â”‚
|    â”œâ”€â”€ features
|        â”œâ”€â”€ feature_building.py     <- Code to create features for modeling
|        â”‚
|    â”œâ”€â”€ model
|        â”œâ”€â”€ model_building.py            <- Code to train models
|        â”‚
|        â”œâ”€â”€ model_evaluation.py          <- Code to run model inference with trained models
|        â”‚
|        â”œâ”€â”€ model_registry.py            <- Code to register model
|        â”‚
   
```

--------


#### DVC PIPELINE

![alt text](image.png)

#### Frontend View (Ex.)


![image](https://github.com/user-attachments/assets/db50670c-2ade-4452-b1c6-73ceb69b74f8)

![image](https://github.com/user-attachments/assets/c987fea5-313a-48b5-b471-db18c59578a3)

![image](https://github.com/user-attachments/assets/a0eb5133-8268-4a54-b332-5e985fdeff7d)


#### Environment Set (Local)

- conda create -n yt-insights-ch-plugin python=3.11
- conda activate yt-insights-ch-plugin
- pip install -r requirements.txt
- git init
- git remote add 
- git add .
- git commit -m "initial commit"
- git push
- dvc init

#### AWS IAM User

- Create a new user with administrator access in aws and get its access key and secret key and region
- Install AWS cli locally in terminal with boto 3 and run aws configure to set your ACCESS and SECRET ACCESS Key and REGION

#### Setting Up EC2 instance for MLFlow

ï»¿ï»¿- sudo apt update: Updates the package list.
ï»¿ï»¿- sudo apt install python3-pip: Installs pip for Python 3.
ï»¿ï»¿- sudo apt install pipx: Installs pipx for managing Python applications.
- ï»¿ï»¿sudo pipx ensurepath: Ensures pipx path is added to the environment.
ï»¿ï»¿- pipx install pipenv: Installs pipenv using pipx.
ï»¿ï»¿- export PATH=SPATH:/home/ubuntu/local/bin: Temporarily adds pipenv to the PATH.
ï»¿ï»¿- echo 'export PATH=$PATH:/home/ubuntu/local/bin'>>~/.bashre: Permanently adds pipenv to the PATH.
ï»¿ï»¿- source ~/.bashre: Applies the changes made to bashre.
ï»¿ï»¿- mkdir miflow: Creates a new directory for the project.
- ï»¿ï»¿cd miflow: Navigates into the project directory.
- pipenv shell: Creates and activates a new virtual environment.
- ï»¿ï»¿pipenv install setuptools: Installs setuptools to ensure pkg_resources is available.
ï»¿ï»¿- pipenv install miflow: Installs MLflow.
- pipenv install awscli: Installs AWS CLI.
- pipenv install boto3: Installs Botos for AWS SDK..
- aws configure: Configures AWS credentials.(your AWS IAM credentials)
- Aws-key : 
- Aws-secret-key : 
- Aws-region : 
- Mlflow server -h 0.0.0.0 â€“default-artifiact-root s3://your-s3-storage-name : Starts the MLFlow server on EC2 instance
- Set following in code :
ï»¿    - ï»¿export MLFLOW _TRACKING _URI=http://:5000/: Sets the MLflow tracking URI in the terminal.
ï»¿ï»¿    - miflow.set _tracking_uri("http://:5000/"'): Sets the MLflow tracking URI in your code.

#### CodeDeploy Launch Template

```bash
#!/bin/bash

# Update the package list
sudo apt-get update -y

# Install Ruby (required by the CodeDeploy agent)
sudo apt-get install ruby -y

# Download the CodeDeploy agent installer from the correct region
wget https://aws-codedeploy-ap-southeast-2.s3.ap-southeast-2.amazonaws.com/latest/install

# Make the installer executable
chmod +x ./install

# Install the CodeDeploy agent
sudo ./install auto

# Start the CodeDeploy agent
sudo service codedeploy-agent start

sudo service codedeploy-agent status	# on asg instance connect command line
```

#### commands to run on ec2 machine connect terminal (yt-plugin-ec2-ecr-cicd) :

	1. sudo apt-get update
	2. sudo apt-get install -y docker.io
	3. sudo systemctl start docker
	4. sudo systemctl enable docker
	5. sudo apt-get install -y unzip curl
	6. curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
	7. unzip awscliv2.zip
	8. sudo ./aws/install
	9. sudo usermod -aG docker ubuntu
	10. exit
	11. reconnect to terminal to check and type : docker --version

#### Docker run command

docker run -p 8888:5000 -e AWS_ACCESS_KEY_ID=YOUR-ACCESS_KEY -e AWS_SECRET_ACCESS_KEY=YOUR-SECRET-ACCESS_KEY niweshbaraj/yt-insights-ch-plugin
